%%
%% This is file `sample-acmsmall-biblatex.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `acmsmall-biblatex')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall-biblatex.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[acmsmall,natbib=false]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}


%%
%% These commands are for a JOURNAL article.
\acmJournal{JACM}
\acmVolume{37}
\acmNumber{4}
\acmArticle{111}
\acmMonth{8}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%


%%
%% The majority of ACM publications use numbered citations and
%% references, obtained by selecting the acmnumeric BibLaTeX style.
%% The acmauthoryear BibLaTeX style switches to the "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the acmauthoryear style of
%% citations and references.
%%
%% Bibliography style
\RequirePackage[
  datamodel=acmdatamodel,
  style=acmauthoryear,
  ]{biblatex}

%% Declare bibliography sources (one \addbibresource command per source)
\addbibresource{software.bib}
\addbibresource{sample-base.bib}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{The Name of the Title Is Hope}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Yucheng Jin}
\authornote{Both authors contributed equally to this research.}
\email{trovato@corporation.com}
\orcid{1234-5678-9012}
\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Fudan University}
  \streetaddress{P.O. Box 1212}
  \city{Shanghai}
  \state{Shanghai}
  \country{China}
  \postcode{43017-6221}
}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
  \institution{Inria Paris-Rocquencourt}
  \city{Rocquencourt}
  \country{France}
}

\author{Aparna Patel}
\affiliation{%
 \institution{Rajiv Gandhi University}
 \streetaddress{Rono-Hills}
 \city{Doimukh}
 \state{Arunachal Pradesh}
 \country{India}}

\author{Huifen Chan}
\affiliation{%
  \institution{Tsinghua University}
  \streetaddress{30 Shuangqing Rd}
  \city{Haidian Qu}
  \state{Beijing Shi}
  \country{China}}

\author{Charles Palmer}
\affiliation{%
  \institution{Palmer Research Laboratories}
  \streetaddress{8600 Datapoint Drive}
  \city{San Antonio}
  \state{Texas}
  \country{USA}
  \postcode{78229}}
\email{cpalmer@prl.com}

\author{John Smith}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{jsmith@affiliation.org}

\author{Julius P. Kumquat}
\affiliation{%
  \institution{The Kumquat Consortium}
  \city{New York}
  \country{USA}}
\email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  A clear and well-documented \LaTeX\ document is presented as an
  article formatted for publication by ACM in a conference proceedings
  or journal publication. Based on the ``acmart'' document class, this
  article presents and explains many of the common variations, as well
  as many of the formatting elements an author may use in the
  preparation of the documentation of their work.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{datasets, neural networks, gaze detection, text tagging}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle
\section{Preliminaries}
Consider a graph $G = (V, E)$ with $N$ nodes, where $V = {v_1, v_2, \ldots, v_N}$ denotes the set of nodes and $E \subset V \times V$ denotes the set of links. Let $A \in {0,1}^{N \times N}$ be the adjacency matrix, where $A_{i,j} = 1$ indicates that nodes $v_i$ and $v_j$ are connected, and $0$ otherwise. Let $X \in \mathbb{R}^{N \times F}$ be the node feature matrix, where $F$ is the number of raw node features and $x_i$ represents the feature vector of node $v_i$ (i.e., the $i$-th row of $X$). We use $y$ to denote the label of each sample, which can be a node, edge, or graph depending on the task. We use a tilde symbol to denote the data generated by GDA methods, for instance, $\tilde{A}$ denotes the augmented adjacency matrix, $\tilde{x}_i$ denotes the augmented feature vector of node $v_i$, and so forth.

Data augmentation (DA) refers to a set of techniques used to increase or generate training data without the need to collect or label more data. Common DA techniques involve creating slightly modified copies of existing data or generating synthetic data based on existing data. By doing so, the augmented data can help prevent overfitting in data-driven models \cite{85}. Commonly used augmentation operations in machine learning, computer vision (CV) \cite{15}, and natural language processing (NLP) \cite{26} include cropping, flipping, and back-translation. In contrast, geometric machine learning (GML) deals with non-Euclidean and irregular data, often represented by graphs. Unlike regular data, such as grids (e.g., images) and sequences (e.g., sentences), structured augmentation operations commonly used in CV and NLP cannot be easily applied to graph data. In node-level and edge-level tasks, objects are interconnected and non-i.i.d., which means that GDA techniques typically modify the entire dataset (graph) instead of a specific data object (nodes or edges) in isolation. A GDA method can generally be defined as a transformation function $f: G \rightarrow \tilde{G}$. The transformation function $f$ can be either rule-based or learnable, and the augmented graph $\tilde{G}$ contains the augmented adjacency matrix $\tilde{A}$ and node feature matrix $\tilde{X}$ (and optionally augmented edge features, node or graph labels). The augmentation function $f$ is not necessarily deterministic, meaning that the same $f$ may generate multiple versions of the augmented graph $\tilde{G}$, and the model may use one or multiple of these augmentations as required for training.

\subsection{Graph Adversarial Training}
Adversarial training is a widely used defense mechanism against adversarial attacks in computer vision \cite{31}, and has also been extended to the graph domain \cite{10, 17, 18, 20, 25, 41, 57}. Unlike graph structure learning, graph adversarial training does not aim to find an optimal graph structure. Instead, it augments input graphs with adversarial patterns during model training by perturbing node features or graph structure. The resulting adversarially trained models are expected to tolerate adversarial perturbations in graph data, leading to better generalization and robustness performance at test time. At the core of adversarial training is the injection of adversarial examples into the training set, which enables the trained model to properly predict test adversarial examples. Therefore, we can use this strategy to enhance the robustness of GNNs as follows:
\begin{equation}
  \min_{\theta} \max_{\Delta_A \in P_A \Delta_X \in P_X} L_{train}(g_{\theta}(A+\Delta_A,X+\Delta_X))
\end{equation}
where $L_{train}$ denotes the training loss for the downstream task; $\Delta_A$ and $\Delta_X$ stand for the perturbation on $A$, $X$,respectively; $P_A$ and $P_X$ denote the perturbation space. From the bi-level optimization problem in Equation (11), we can observe that adversarial training generates perturbations that maximize the prediction loss and updates model parameters to minimize the prediction loss. The process of generating perturbations (i.e., $A+\Delta_A,X+\Delta_X$) can be viewed as adversarial data augmentation and we can leverage such augmentations to improve the model robustness and generalization.

Dai et al. \cite{17} proposed a simple adversarial training strategy for augmenting the adjacency matrix by randomly dropping edges during training, without any optimization on the graph data. Although this approach does not provide significant improvement, it still shows some benefits in enhancing the robustness of GNNs. This finding is consistent with that of Z ̈ugner and G ̈unnemann \cite{146}. On the other hand, Xu et al. \cite{116} utilized projected gradient descent (PGD) to optimize the bi-level problem and generate perturbations on the discrete structure, achieving a significant improvement in robust performance. Similarly, Chen et al. \cite{9} and Dai et al. \cite{18} used existing adversarial attacks to modify the input graph structure during adversarial training for network embedding methods. Additionally, Suresh et al. \cite{92} proposed to learn to drop edges during graph augmentation to capture the minimal information needed to classify each graph.

In contrast to augmenting the adjacency matrix, some works focus on perturbing the input features as adversarial examples. For example, Feng et al. \cite{25} proposed dynamic regularization to reconstruct graph smoothness and constrain the divergence between the prediction of the target node and its connected nodes. Deng et al. \cite{20} introduced batch virtual adversarial training to promote the smoothness of GNNs and improve their resilience against adversarial perturbations. Kong et al. \cite{57} developed FLAG, which utilizes adversarial training to iteratively augment the node features with gradient-based adversarial perturbations, and enhances the performance of GNNs in node classification, link prediction, and graph classification tasks. Moreover, Z ̈ugner and G ̈unnemann \cite{145} investigated the certifiable robustness of GNNs with respect to perturbations of node attributes and proposed a robust training scheme inspired by the certificates. Several other variants of adversarial training on perturbing node features are introduced in \cite{41, 103}.

\subsection{Rationalization}
Rationalization in machine learning refers to the process of explaining model predictions by identifying the input features that are most relevant to the prediction. In the graph domain, rationalization is typically implemented as a form of augmented graph data that includes intrinsically learned subgraphs representing key features of the graph. These subgraphs are used to inform model decisions and provide explanations for the model's predictions. Rationalization is commonly applied to graph property prediction or graph classification tasks, such as drug discovery, molecular and polymer datasets, etc.

Early rationalization methods in the graph domain focused on improving interpretability and general graph classification performance. Yu et al. \cite{121} proposed the Graph Information bottleneck framework (GIB) that learns to generate maximally informative and compressed subgraphs by leveraging a bi-level optimization scheme and a novel connectivity loss. Miao et al. \cite{73} proposed GSAT to better learn and select task-relevant subgraphs by injecting stochasticity into the attention weights to constrain information from task-irrelevant components. Liu et al. \cite{64} proposed GREA, a rationalization method that improves rationale identification by separating the rationale and environment and subsequently replacing the environment to generate augmented virtual data examples.

Rationalization models also help solve data bias and out-of-distribution (OOD) problems for graph property prediction tasks, providing better interpretation and generalization. Wu et al. \cite{111} proposed DIR to generate distribution perturbation on training data with causal intervention. They created a rationale generator that separates causal and non-causal graphs, applies causal intervention to create perturbed distributions, and then jointly learn both the causal and non-causal representation to minimize invariant risk. Similarly, Chen et al. \cite{12} proposed CIGA to model the graph generation process and the interactions between invariant and spurious features with Structural Causal Models (SCM). The resulting subgraphs generated by CIGA maximally preserves the invariant intra-class information. Li et al. \cite{62} proposed GIL, a GNN-based subgraph generator that identifies potentially invariant subgraphs, infers latent environment labels for variant subgraphs, and jointly optimizes all modules. 

In summary, rationalization methods in the graph domain are intrinsically interpretable models that include a rationalization component in the model, as opposed to post-hoc explanation methods. These methods help improve interpretability, generalization, and solve data bias and OOD problems for graph property prediction tasks.

\printbibliography

%%
%% If your work has an appendix, this is the place to put it.
\appendix

% \section{Research Methods}

% \subsection{Part One}

% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
% malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
% sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
% vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
% lacinia dolor. Integer ultricies commodo sem nec semper.

% \subsection{Part Two}

% Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
% ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
% ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
% eros. Vivamus non purus placerat, scelerisque diam eu, cursus
% ante. Etiam aliquam tortor auctor efficitur mattis.

% \section{Online Resources}

% Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
% pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
% enim maximus. Vestibulum gravida massa ut felis suscipit
% congue. Quisque mattis elit a risus ultrices commodo venenatis eget
% dui. Etiam sagittis eleifend elementum.

% Nam interdum magna at lectus dignissim, ac dignissim lorem
% rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
% massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-acmsmall-biblatex.tex'.
